{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 华为算法精英赛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据（`user_app_usage.csv` 除外）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_train = pd.read_csv(\"age_train.csv\", names=['uid','age_group'])\n",
    "age_test = pd.read_csv(\"age_test.csv\", names=['uid'])\n",
    "user_basic_info = pd.read_csv(\"user_basic_info.csv\", names=['uid','gender','city','prodName','ramCapacity','ramLeftRation','romCapacity','romLeftRation','color','fontSize','ct','carrier','os'])\n",
    "user_behavior_info = pd.read_csv(\"user_behavior_info.csv\", names=['uid','bootTimes','AFuncTimes','BFuncTimes','CFuncTimes','DFuncTimes','EFuncTimes','FFuncTimes','FFuncSum'])\n",
    "user_app_actived = pd.read_csv(\"user_app_actived.csv\", names=['uid','appId'])\n",
    "app_info = pd.read_csv(\"app_info.csv\", names=['appId', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2id = {}\n",
    "id2class = {}\n",
    "\n",
    "def mergeBasicTables(baseTable):\n",
    "    \n",
    "    resTable = baseTable.merge(user_basic_info, how='left', on='uid', suffixes=('_base0', '_ubaf'))\n",
    "    resTable = resTable.merge(user_behavior_info, how='left', on='uid', suffixes=('_base1', '_ubef'))\n",
    "    \n",
    "    cat_columns = ['city','prodName','color','carrier','os','ct']\n",
    "    for c in cat_columns:\n",
    "        resTable[c] = resTable[c].apply(lambda x: x if type(x)==str else str(x))\n",
    "        \n",
    "        sort_temp = sorted(list(set(resTable[c])))  \n",
    "        class2id[c+'2id'] = dict(zip(sort_temp, range(1, len(sort_temp)+1)))\n",
    "        id2class['id2'+c] = dict(zip(range(1,len(sort_temp)+1), sort_temp))\n",
    "        resTable[c] = resTable[c].apply(lambda x: class2id[c+'2id'][x])\n",
    "        \n",
    "    return resTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic_info = mergeBasicTables(age_train)\n",
    "test_basic_info = mergeBasicTables(age_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic_info.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`user_basic_info.csv` 中的 `ramCapacity`，`ramLeftRation`，`romCapacity`，`romLeftRation` 和 `fontSize` 列都包含 `NAN` 值，采用平均值进行填充 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic_info['ramCapacity'] = train_basic_info['ramCapacity'].fillna(round(user_basic_info['ramCapacity'].mean()))\n",
    "test_basic_info['ramCapacity'] = test_basic_info['ramCapacity'].fillna(round(user_basic_info['ramCapacity'].mean()))\n",
    "\n",
    "train_basic_info['ramLeftRation'] = train_basic_info['ramLeftRation'].fillna(round(user_basic_info['ramLeftRation'].mean()))\n",
    "test_basic_info['ramLeftRation'] = test_basic_info['ramLeftRation'].fillna(round(user_basic_info['ramLeftRation'].mean(), 2))\n",
    "\n",
    "train_basic_info['romCapacity'] = train_basic_info['romCapacity'].fillna(round(user_basic_info['romCapacity'].mean()))\n",
    "test_basic_info['romCapacity'] = test_basic_info['romCapacity'].fillna(round(user_basic_info['romCapacity'].mean()))\n",
    "\n",
    "train_basic_info['romLeftRation'] = train_basic_info['romLeftRation'].fillna(round(user_basic_info['romLeftRation'].mean(), 2))\n",
    "test_basic_info['romLeftRation'] = test_basic_info['romLeftRation'].fillna(round(user_basic_info['romLeftRation'].mean(), 2))\n",
    "\n",
    "train_basic_info['fontSize'] = train_basic_info['fontSize'].fillna(round(user_basic_info['fontSize'].mean(), 2))\n",
    "test_basic_info['fontSize'] = test_basic_info['fontSize'].fillna(round(user_basic_info['fontSize'].mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic_info.to_csv('train_basic_info.csv')\n",
    "test_basic_info.to_csv('test_basic_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic_info = pd.read_csv('./data/train_basic_info.csv')\n",
    "test_basic_info = pd.read_csv('./data/test_basic_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_app_info = train_basic_info.merge(user_app_actived, how='left', on='uid')[['uid', 'appId']]\n",
    "test_app_info = test_basic_info.merge(user_app_actived, how='left', on='uid')[['uid', 'appId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_info_dict = {}\n",
    "for i in range(len(app_info)):\n",
    "    entry = app_info.iloc[i]\n",
    "    if entry.appId not in app_info_dict:\n",
    "        app_info_dict[entry.appId] = entry.category\n",
    "    else:\n",
    "        app_info_dict[entry.appId] = app_info_dict[entry.appId] + '#' + entry.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCategoryStr(appIdStr):\n",
    "    categoryList = []\n",
    "    appList = appIdStr.split('#')\n",
    "    for i in appList:\n",
    "        if i in app_info_dict:\n",
    "            categoryList.append(app_info_dict[i])\n",
    "    return '#'.join(categoryList)\n",
    "\n",
    "train_app_info['categoryId'] = train_app_info.appId.apply(getCategoryStr)\n",
    "test_app_info['categoryId'] = test_app_info.appId.apply(getCategoryStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>appId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000001</td>\n",
       "      <td>a00140327#a00170298#a00184278#a00187480#a00239...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000011</td>\n",
       "      <td>a00158535#a00163116#a00170432#a00187480#a00224...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid                                              appId\n",
       "0  1000001  a00140327#a00170298#a00184278#a00187480#a00239...\n",
       "1  1000011  a00158535#a00163116#a00170432#a00187480#a00224..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_app_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_app_info['categoryId'] = train_app_info.appId.apply(getCategoryStr)\n",
    "test_app_info['categoryId'] = test_app_info.appId.apply(getCategoryStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(set(app_info['category']))\n",
    "category2id = dict(zip(sorted(cats), range(0,len(cats))))\n",
    "id2category = dict(zip(range(0,len(cats)), sorted(cats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopNCategory(categoryStr, n):\n",
    "    tops = pd.value_counts(categoryStr.split('#'))\n",
    "    ret = []\n",
    "    for i in range(1, n+1):\n",
    "        if categoryStr.split('#')[0] == '' or len(tops) < i:\n",
    "            ret.append(str(40))\n",
    "        else:\n",
    "            ret.append(str(category2id[tops.index[i - 1]]))\n",
    "            \n",
    "    return '#'.join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_app_info['top3Category'] = train_app_info.categoryId.apply(lambda x: getTopNCategory(x, 3))\n",
    "test_app_info['top3Category'] = test_app_info.categoryId.apply(lambda x: getTopNCategory(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopNCategory(categoryStr, n):\n",
    "    tops = pd.value_counts(categoryStr.split('#'))\n",
    "    ret = []\n",
    "    for i in range(1, n+1):\n",
    "        if categoryStr.split('#')[0] == '' or len(tops) < i:\n",
    "            ret.append(str(40))\n",
    "        else:\n",
    "            ret.append(str(category2id[tops.index[i - 1]]))\n",
    "            \n",
    "    return '#'.join(ret)\n",
    "\n",
    "train_app_info['top3Category'] = train_app_info.categoryId.apply(lambda x: getTopNCategory(x, 3))\n",
    "test_app_info['top3Category'] = test_app_info.categoryId.apply(lambda x: getTopNCategory(x, 3))\n",
    "\n",
    "train_app_info['1stCategory'] = train_app_info['top3Category'].apply(lambda x: int(x.split('#')[0]))\n",
    "train_app_info['2ndCategory'] = train_app_info['top3Category'].apply(lambda x: int(x.split('#')[1]))\n",
    "train_app_info['3rdCategory'] = train_app_info['top3Category'].apply(lambda x: int(x.split('#')[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_app_info['1stCategory'] = test_app_info['top3Category'].apply(lambda x: int(x.split('#')[0]))\n",
    "test_app_info['2ndCategory'] = test_app_info['top3Category'].apply(lambda x: int(x.split('#')[1]))\n",
    "test_app_info['3rdCategory'] = test_app_info['top3Category'].apply(lambda x: int(x.split('#')[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_app_info.to_csv('train_app_info.csv')\n",
    "test_app_info.to_csv('test_app_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_app_info = pd.read_csv('./data/train_app_info.csv')\n",
    "test_app_info = pd.read_csv('./data/test_app_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countVzer = CountVectorizer(min_df=1, max_df=0.7, tokenizer=lambda x: x.split('#'), max_features=3000)\n",
    "train_app_counts = countVzer.fit_transform(train_app_info['appId'])\n",
    "test_app_counts = countVzer.transform(test_app_info['appId'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidfTfmer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
    "train_app_tfidfs = tfidfTfmer.fit_transform(train_app_counts)\n",
    "test_app_tfidfs = tfidfTfmer.transform(test_app_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidfTfmer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
    "train_app_tfidfs = tfidfTfmer.fit_transform(train_app_counts)\n",
    "test_app_tfidfs = tfidfTfmer.transform(test_app_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_app_info['categoryId'] = train_app_info['categoryId'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_app_info['categoryId'] = test_app_info['categoryId'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVzer = CountVectorizer(min_df=0, max_df=1.0, tokenizer=lambda x: x.split('#'))\n",
    "train_category_counts = countVzer.fit_transform(train_app_info['categoryId'])\n",
    "test_category_counts = countVzer.transform(test_app_info['categoryId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category_tfidfs = tfidfTfmer.fit_transform(train_category_counts)\n",
    "test_category_tfidfs = tfidfTfmer.transform(test_category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.LinearSVC(C=0.01)\n",
    "SVM.fit(train_app_counts, train_basic_info['age_group'])\n",
    "SVM_preds = SVM.predict(test_app_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':test_basic_info['uid'],'label':SVM_preds})\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.56559999999999999"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.LinearSVC(C=0.007)\n",
    "SVM.fit(sp.hstack((train_app_counts[:50000], train_category_counts[:50000]), format='csr'), train_basic_info['age_group'][:50000])\n",
    "SVM_preds = SVM.predict(sp.hstack((train_app_counts[-5000:], train_category_counts[-5000:]), format='csr'))\n",
    "accuracy_score(SVM_preds, train_basic_info['age_group'][-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55940000000000001"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = svm.LinearSVC(C=0.08)\n",
    "SVM.fit(train_app_tfidfs[:50000], train_basic_info['age_group'][:50000])\n",
    "SVM_preds = SVM.predict(train_app_tfidfs[-5000:])\n",
    "accuracy_score(SVM_preds, train_basic_info['age_group'][-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56359999999999999"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.LinearSVC(C=0.008)\n",
    "SVM.fit(train_app_counts[:50000], train_basic_info['age_group'][:50000])\n",
    "SVM_preds = SVM.predict(train_app_counts[-5000:])\n",
    "accuracy_score(SVM_preds, train_basic_info['age_group'][-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x3001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 353329 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.hstack((csr_matrix(train_basic_info[['age_group']])[:10000], train_app_tfidfs[:10000]), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_info = csr_matrix((train_basic_info[['age_group']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80500000000000005"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = svm.LinearSVC(C=0.03)\n",
    "SVM.fit(sp.hstack((age_info[:10000], train_app_tfidfs[:10000]), format='csr'), train_basic_info['age_group'][:10000])\n",
    "SVM_preds = SVM.predict(sp.hstack((age_info[-5000:], train_app_tfidfs[-5000:]), format='csr'))\n",
    "accuracy_score(SVM_preds, train_basic_info['age_group'][-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 51],\n",
       "       [ 30],\n",
       "       [228],\n",
       "       ..., \n",
       "       [ 84],\n",
       "       [ 30],\n",
       "       [143]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_basic_info[['city']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.13850416],\n",
       "       [ 0.08033241],\n",
       "       [ 0.62880886],\n",
       "       ..., \n",
       "       [ 0.2299169 ],\n",
       "       [ 0.08033241],\n",
       "       [ 0.3933518 ]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit_transform(train_basic_info[['city']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_basic_info['age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, n, k):\n",
    "        super(FM, self).__init__()\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.linear = nn.Linear(self.n, 6)   # 前两项线性层\n",
    "        self.V = nn.Parameter(torch.randn(self.n, self.k))   # 交互矩阵\n",
    "    def fm_layer(self, x):\n",
    "        linear_part = self.linear(x)\n",
    "        interaction_part_1 = torch.mm(x, self.V)\n",
    "        interaction_part_1 = torch.pow(interaction_part_1, 2)\n",
    "        interaction_part_2 = torch.mm(torch.pow(x, 2), torch.pow(self.V, 2))\n",
    "        output = linear_part + torch.sum(0.5 * interaction_part_2 - interaction_part_1)\n",
    "        \n",
    "        return output\n",
    "    def forward(self, x):\n",
    "        return self.fm_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2010000x9395 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 73110632 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_app_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold[1]\n",
      "Batch[0] Train loss: 1.8029\tTrain acc: 0.0820\n",
      "Batch[10] Train loss: 1.7351\tTrain acc: 0.3203\n",
      "Batch[20] Train loss: 1.6660\tTrain acc: 0.3516\n",
      "Batch[30] Train loss: 1.6429\tTrain acc: 0.3398\n",
      "Batch[40] Train loss: 1.5813\tTrain acc: 0.3438\n",
      "Batch[50] Train loss: 1.5449\tTrain acc: 0.4258\n",
      "Batch[60] Train loss: 1.5394\tTrain acc: 0.4102\n",
      "Batch[70] Train loss: 1.5002\tTrain acc: 0.4453\n",
      "Batch[80] Train loss: 1.4719\tTrain acc: 0.4688\n",
      "Batch[90] Train loss: 1.4712\tTrain acc: 0.4531\n",
      "Batch[100] Train loss: 1.4630\tTrain acc: 0.4648\n",
      "Batch[110] Train loss: 1.4070\tTrain acc: 0.4766\n",
      "Batch[120] Train loss: 1.4323\tTrain acc: 0.4609\n",
      "Batch[130] Train loss: 1.4056\tTrain acc: 0.4922\n",
      "Batch[140] Train loss: 1.4103\tTrain acc: 0.4883\n",
      "Batch[150] Train loss: 1.3680\tTrain acc: 0.5195\n",
      "Batch[160] Train loss: 1.3982\tTrain acc: 0.4844\n",
      "Batch[170] Train loss: 1.3856\tTrain acc: 0.4727\n",
      "Batch[180] Train loss: 1.3777\tTrain acc: 0.5117\n",
      "Batch[190] Train loss: 1.3474\tTrain acc: 0.5195\n",
      "Batch[200] Train loss: 1.3527\tTrain acc: 0.4570\n",
      "Batch[210] Train loss: 1.2993\tTrain acc: 0.5430\n",
      "Batch[220] Train loss: 1.2828\tTrain acc: 0.5078\n",
      "Batch[230] Train loss: 1.2968\tTrain acc: 0.4883\n",
      "Batch[240] Train loss: 1.3002\tTrain acc: 0.5234\n",
      "Batch[250] Train loss: 1.3070\tTrain acc: 0.4883\n",
      "Epoch[1/5] Train loss: 1.4455\tTrain acc: 0.4529\n",
      "           Test loss: 1.5292\tTest acc: 0.3060\n",
      "Batch[0] Train loss: 1.2274\tTrain acc: 0.5391\n",
      "Batch[10] Train loss: 1.2986\tTrain acc: 0.4766\n",
      "Batch[20] Train loss: 1.2778\tTrain acc: 0.4922\n",
      "Batch[30] Train loss: 1.2983\tTrain acc: 0.5312\n",
      "Batch[40] Train loss: 1.2419\tTrain acc: 0.5547\n",
      "Batch[50] Train loss: 1.2501\tTrain acc: 0.5391\n",
      "Batch[60] Train loss: 1.2468\tTrain acc: 0.5742\n",
      "Batch[70] Train loss: 1.2031\tTrain acc: 0.5391\n",
      "Batch[80] Train loss: 1.2335\tTrain acc: 0.5508\n",
      "Batch[90] Train loss: 1.2244\tTrain acc: 0.5273\n",
      "Batch[100] Train loss: 1.2407\tTrain acc: 0.5312\n",
      "Batch[110] Train loss: 1.1670\tTrain acc: 0.5625\n",
      "Batch[120] Train loss: 1.2126\tTrain acc: 0.5195\n",
      "Batch[130] Train loss: 1.2058\tTrain acc: 0.5703\n",
      "Batch[140] Train loss: 1.1946\tTrain acc: 0.5703\n",
      "Batch[150] Train loss: 1.1764\tTrain acc: 0.5820\n",
      "Batch[160] Train loss: 1.2188\tTrain acc: 0.5430\n",
      "Batch[170] Train loss: 1.2061\tTrain acc: 0.5312\n",
      "Batch[180] Train loss: 1.1997\tTrain acc: 0.5703\n",
      "Batch[190] Train loss: 1.1619\tTrain acc: 0.5859\n",
      "Batch[200] Train loss: 1.1934\tTrain acc: 0.5273\n",
      "Batch[210] Train loss: 1.1492\tTrain acc: 0.5703\n",
      "Batch[220] Train loss: 1.1223\tTrain acc: 0.5898\n",
      "Batch[230] Train loss: 1.1505\tTrain acc: 0.5469\n",
      "Batch[240] Train loss: 1.1560\tTrain acc: 0.5625\n",
      "Batch[250] Train loss: 1.1777\tTrain acc: 0.5352\n",
      "Epoch[2/5] Train loss: 1.2105\tTrain acc: 0.5426\n",
      "           Test loss: 1.2778\tTest acc: 0.3859\n",
      "Batch[0] Train loss: 1.1033\tTrain acc: 0.5742\n",
      "Batch[10] Train loss: 1.1626\tTrain acc: 0.5156\n",
      "Batch[20] Train loss: 1.1682\tTrain acc: 0.5234\n",
      "Batch[30] Train loss: 1.1765\tTrain acc: 0.5703\n",
      "Batch[40] Train loss: 1.1401\tTrain acc: 0.5898\n",
      "Batch[50] Train loss: 1.1353\tTrain acc: 0.5312\n",
      "Batch[60] Train loss: 1.1353\tTrain acc: 0.6094\n",
      "Batch[70] Train loss: 1.0898\tTrain acc: 0.5859\n",
      "Batch[80] Train loss: 1.1494\tTrain acc: 0.5586\n",
      "Batch[90] Train loss: 1.1272\tTrain acc: 0.5469\n",
      "Batch[100] Train loss: 1.1438\tTrain acc: 0.5547\n",
      "Batch[110] Train loss: 1.0608\tTrain acc: 0.5781\n",
      "Batch[120] Train loss: 1.1167\tTrain acc: 0.5508\n",
      "Batch[130] Train loss: 1.1169\tTrain acc: 0.5859\n",
      "Batch[140] Train loss: 1.0969\tTrain acc: 0.5938\n",
      "Batch[150] Train loss: 1.0876\tTrain acc: 0.6055\n",
      "Batch[160] Train loss: 1.1262\tTrain acc: 0.5977\n",
      "Batch[170] Train loss: 1.1138\tTrain acc: 0.5508\n",
      "Batch[180] Train loss: 1.1162\tTrain acc: 0.6172\n",
      "Batch[190] Train loss: 1.0720\tTrain acc: 0.6250\n",
      "Batch[200] Train loss: 1.1116\tTrain acc: 0.5664\n",
      "Batch[210] Train loss: 1.0754\tTrain acc: 0.5742\n",
      "Batch[220] Train loss: 1.0430\tTrain acc: 0.6094\n",
      "Batch[230] Train loss: 1.0752\tTrain acc: 0.5625\n",
      "Batch[240] Train loss: 1.0835\tTrain acc: 0.5898\n",
      "Batch[250] Train loss: 1.1101\tTrain acc: 0.5508\n",
      "Epoch[3/5] Train loss: 1.1168\tTrain acc: 0.5701\n",
      "           Test loss: 1.1432\tTest acc: 0.4262\n",
      "Batch[0] Train loss: 1.0396\tTrain acc: 0.6055\n",
      "Batch[10] Train loss: 1.0923\tTrain acc: 0.5391\n",
      "Batch[20] Train loss: 1.1074\tTrain acc: 0.5430\n",
      "Batch[30] Train loss: 1.1130\tTrain acc: 0.5703\n",
      "Batch[40] Train loss: 1.0862\tTrain acc: 0.5820\n",
      "Batch[50] Train loss: 1.0715\tTrain acc: 0.5547\n",
      "Batch[60] Train loss: 1.0754\tTrain acc: 0.6250\n",
      "Batch[70] Train loss: 1.0266\tTrain acc: 0.6055\n",
      "Batch[80] Train loss: 1.1038\tTrain acc: 0.5703\n",
      "Batch[90] Train loss: 1.0740\tTrain acc: 0.5742\n",
      "Batch[100] Train loss: 1.0850\tTrain acc: 0.5859\n",
      "Batch[110] Train loss: 0.9985\tTrain acc: 0.6094\n",
      "Batch[120] Train loss: 1.0623\tTrain acc: 0.5625\n",
      "Batch[130] Train loss: 1.0652\tTrain acc: 0.5898\n",
      "Batch[140] Train loss: 1.0422\tTrain acc: 0.6172\n",
      "Batch[150] Train loss: 1.0347\tTrain acc: 0.6211\n",
      "Batch[160] Train loss: 1.0693\tTrain acc: 0.5898\n",
      "Batch[170] Train loss: 1.0635\tTrain acc: 0.5586\n",
      "Batch[180] Train loss: 1.0686\tTrain acc: 0.6328\n",
      "Batch[190] Train loss: 1.0173\tTrain acc: 0.6445\n",
      "Batch[200] Train loss: 1.0576\tTrain acc: 0.6055\n",
      "Batch[210] Train loss: 1.0278\tTrain acc: 0.5781\n",
      "Batch[220] Train loss: 1.0015\tTrain acc: 0.6289\n",
      "Batch[230] Train loss: 1.0256\tTrain acc: 0.6016\n",
      "Batch[240] Train loss: 1.0361\tTrain acc: 0.5977\n",
      "Batch[250] Train loss: 1.0674\tTrain acc: 0.5391\n",
      "Epoch[4/5] Train loss: 1.0625\tTrain acc: 0.5867\n",
      "           Test loss: 0.1536\tTest acc: 0.2535\n",
      "Batch[0] Train loss: 0.9990\tTrain acc: 0.6211\n",
      "Batch[10] Train loss: 1.0444\tTrain acc: 0.5898\n",
      "Batch[20] Train loss: 1.0693\tTrain acc: 0.5703\n",
      "Batch[30] Train loss: 1.0674\tTrain acc: 0.5820\n",
      "Batch[40] Train loss: 1.0508\tTrain acc: 0.6172\n",
      "Batch[50] Train loss: 1.0317\tTrain acc: 0.5781\n",
      "Batch[60] Train loss: 1.0400\tTrain acc: 0.6328\n",
      "Batch[70] Train loss: 0.9854\tTrain acc: 0.6328\n",
      "Batch[80] Train loss: 1.0679\tTrain acc: 0.5625\n",
      "Batch[90] Train loss: 1.0376\tTrain acc: 0.5977\n",
      "Batch[100] Train loss: 1.0435\tTrain acc: 0.6094\n",
      "Batch[110] Train loss: 0.9565\tTrain acc: 0.6367\n",
      "Batch[120] Train loss: 1.0249\tTrain acc: 0.5742\n",
      "Batch[130] Train loss: 1.0312\tTrain acc: 0.6055\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 5\n",
    "batch_size = 256\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-5\n",
    "log_interval = 10\n",
    "k = 100\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "test_acc = 0\n",
    "\n",
    "clf_FM = FM(train_app_counts.shape[1], k)\n",
    "clf_FM.to(device)\n",
    "optimizer = torch.optim.Adam(clf_FM.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "fm_train_losses = []\n",
    "fm_train_accs = []\n",
    "fm_test_losses = []\n",
    "fm_test_accs = []\n",
    "\n",
    "for f, (train_index, val_index) in enumerate(kf.split(train_app_counts[:100000])):\n",
    "    print('Fold[{}]'.format(f+1))\n",
    "    for epoch in range(num_epochs):\n",
    "        clf_FM.train()\n",
    "        fm_loss = 0\n",
    "        fm_acc = 0\n",
    "        for i in range(int(len(train_index)/batch_size)):\n",
    "            batch_train_counts = Variable(torch.FloatTensor(\n",
    "                train_app_counts[:100000][train_index][i*batch_size:i*batch_size+batch_size].toarray())).to(device)\n",
    "            batch_train_targets = Variable(torch.LongTensor(\n",
    "                y_train[:100000][train_index][i*batch_size:i*batch_size+batch_size].values)).to(device)\n",
    "            output = clf_FM(batch_train_counts)\n",
    "            \n",
    "            loss = loss_fn(output, batch_train_targets-1)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_train_predicts = torch.max(output, dim=1)[1]\n",
    "            acc = accuracy_score((batch_train_targets-1).cpu(), batch_train_predicts.cpu())\n",
    "            \n",
    "            fm_loss += loss\n",
    "            fm_acc += acc\n",
    "            if i % log_interval == 0:\n",
    "                print('Batch[{}] Train loss: {:.4f}\\tTrain acc: {:.4f}'.format(i, loss, acc))\n",
    "            \n",
    "        fm_loss = fm_loss/int(len(train_index)/batch_size)\n",
    "        fm_acc = fm_acc/int(len(train_index)/batch_size)\n",
    "        fm_train_losses.append(fm_loss)\n",
    "        fm_train_accs.append(fm_acc)\n",
    "        print('Epoch[{}/{}] Train loss: {:.4f}\\tTrain acc: {:.4f}'.format(epoch+1, num_epochs, fm_loss, fm_acc))\n",
    "        \n",
    "        clf_FM.eval()\n",
    "        with torch.no_grad():\n",
    "            output = clf_FM(torch.FloatTensor(train_app_counts[-10000:].toarray()).to(device))\n",
    "            fm_loss = loss_fn(output, torch.LongTensor(y_train[-10000:].values-1).to(device))\n",
    "            test_predicts = torch.max(output, dim=1)[1]\n",
    "            fm_acc = accuracy_score(torch.LongTensor(y_train[-10000:].values-1).cpu(), test_predicts.cpu())\n",
    "            \n",
    "            fm_test_losses.append(fm_loss)\n",
    "            fm_test_accs.append(fm_acc)\n",
    "            print('           Test loss: {:.4f}\\tTest acc: {:.4f}'.format(fm_loss, fm_acc))\n",
    "            \n",
    "            if (acc > test_acc):\n",
    "                save_model(clf_FM, './state_dict/fm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_FM.load_state_dict(torch.load('./state_dict/fm.pt'))\n",
    "clf_FM.eval()\n",
    "with torch.no_grad():\n",
    "    output = clf_FM(torch.FloatTensor(test_app_counts[:10].toarray()).to(device))\n",
    "    clf_FM_preds = torch.max(output, dim=1)[1].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':test_basic_info['uid'][:10],'label':clf_FM_preds})\n",
    "df.to_csv('asd.csv',index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
